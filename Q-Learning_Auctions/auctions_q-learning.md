### The Model

#### 1st auction

Bandit framework: each bidder only observes the auction outcome of the auction he participated in, his own private value and the number of participants of the auction. The bidders learn by interacting with the environment (the auctionneer, the other bidders and the auction rules), and are model free. This case can be configured as a bandit problem because each bidders doesn't know how he is controlling the environment, only receiving rewards for the actions taken. That is, the bidder observes his private value, drawn by a known distribution, and the number of bidders participating, chooses a policy, from where an action is drawn, and receives an outcome (in the case of sealed first-price auction, that is his bid minus his private value, if he wins, and zero, if he loses). The policy, a probability distribution over actions, is conditional on the observation, that in the simplest case is the private value and the number of participants. None of these are controllable by the bidder. In this simple game the only controllable aspect of the environment is the other bidders' policies, that neither bidder observers. Then, the best each bidder can do is consider the environment as a contextual bandit, where the context is the collective private value distribution and the number of participants.
